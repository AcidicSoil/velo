# Velo

**Localâ€‘first AI CLI assistant powered by **``**â€¯+â€¯DSPy**

Velo brings offline largeâ€‘languageâ€‘model magic right into your terminal and editor. It delivers interactive chat, code completion, naturalâ€‘language â†’ shell, error explanations, and projectâ€‘wide refactorsâ€”all running on your GPU (Vulkan) or CPU, with an optional cloud fallback.

---

## âœ¨ Features

- `` â€“ multiâ€‘turn REPL with Model Context Protocol (MCP) / functionâ€‘calling support (Qwen3, Osmosisâ€‘MCPâ€‘4B, smolLM2, etc.).
- `` â€“ contextâ€‘aware code completion at `FILE:LINE`.
- `` â€“ translate natural language into readyâ€‘toâ€‘run shell commands (`-y` to autoâ€‘execute).
- `` â€“ break down error logs and suggest fixes.
- `` â€“ projectâ€‘wide refactors driven by naturalâ€‘language instructions.
- **Hybrid retrieval (HNSWÂ â†’Â Treeâ€‘Sitter chunks)** for fast, precise context.
- **Vulkan GPU offâ€‘loading** (AMD, NVIDIA, Intel) with automatic CPU fallback.

---

## ğŸ“¦ Installation

| Method                  | Command                                                                   |
| ----------------------- | ------------------------------------------------------------------------- |
| **Python (pipx)**       | `pipx install velo-ai`                                                    |
| **Python (virtualenv)** | `pip install velo-ai`                                                     |
| **Standalone binary**   | Download the latest release for your OS and place `velo` on your `$PATH`. |

> **Requires**: \~8â€¯GB disk for model weights. 16â€¯GB RAM (CPU) or 8â€¯GB VRAM (GPU) recommended.

### Enable Vulkan (recommended)

```bash
# Linux / macOS (CMake build)
cmake -B build -DGGML_USE_VULKAN=ON .. && cmake --build build -j

# Windows / MSYS2 (make build)
LLAMA_VULKAN=1 make -j
```

Velo autoâ€‘detects the `llama.cpp` server at `http://localhost:11434`; set `VELO_API_URL` to override.

---

## ğŸš€ Quick Start

```bash
# 1) First run â€“ launch the interactive wizard
velo                     # seeds caches, picks a default model

# 2) Chat with the assistant
velo chat

# 3) Ask for a shell command
velo shell "list largest git objects" -y

# 4) Complete code at a cursor
velo complete src/foo.py:120
```

The wizard writes perâ€‘project config to `.velo/` and stores heavy assets under `~/.velo/`.

---

## ğŸ”§ Configuration

- `~/.velo/config.toml` â€“ global defaults (model path, retrieval knobs).
- `<repo>/.velo/config.toml` â€“ perâ€‘project overrides.
- CLI flags always override config files.

Common flags:

| Flag                          | Purpose                                           |
| ----------------------------- | ------------------------------------------------- |
| `--model /path/to/model.gguf` | Use a specific local model.                       |
| `--cloud`                     | Route requests to cloud provider (if configured). |
| `--top-files N`               | Adjust coarse retrieval width (defaultÂ 8).        |
| `--quality high`              | Enable crossâ€‘encoder rerank for refactors.        |
| `--json`                      | Machineâ€‘parseable output.                         |

---

## ğŸ—ºï¸ Roadmap

1. VSÂ Code/Cursor inline integration.
2. Git commit hooks for autoâ€‘lint and PR comments.
3. Web UI overlay (Streamlit/Textual).
4. Codeâ€‘Interpreter & Browserâ€‘automation modes via Qwenâ€‘Agent.

See the full [Master Plan](./Masterplan) for details.

---

## ğŸ¤ Contributing

PRs and discussions welcome!  Please read `CONTRIBUTING.md` (coming soon) and open an issue to get started.

---

## ğŸ“„ License

Velo is released under the MIT License.  See `LICENSE` for more information.

